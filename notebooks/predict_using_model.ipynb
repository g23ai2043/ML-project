{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\Billaaa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\Billaaa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "import random\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "import joblib\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from tqdm import tqdm\n",
    "\n",
    "tqdm.pandas()\n",
    "nltk.download('punkt_tab')\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "from settings import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_prep_txt(text):\n",
    "    ls=[]\n",
    "    ls.append(text)\n",
    "    df=pd.DataFrame(ls)\n",
    "    df.columns =['text']\n",
    "\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "# Performing Data Cleaning\n",
    "    df['text'] = df['text'].apply(data_cleaning)\n",
    "    df['tokenized text'] = df['text'].progress_apply(lambda document: word_tokenize(document.strip()))\n",
    "    df['clean_tokens'] = df['tokenized text'].progress_apply(lambda tokens: [token for token in tokens if token not in stop_words])\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Applying Lemmatization\n",
    "    df['lemmatized text'] = df['clean_tokens'].progress_apply(lambda tokens: ' '.join([lemmatizer.lemmatize(token, pos='v') for token in tokens]))\n",
    "\n",
    "# Creating new features\n",
    "    df['no_of_charcters'] = df['lemmatized text'].progress_apply(len)\n",
    "    df['no_of_words'] = df['lemmatized text'].progress_apply(lambda document: word_tokenize(document)).apply(len)\n",
    "    return df['lemmatized text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_prep_series(df):\n",
    "    \n",
    "    df = df\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "# Performing Data Cleaning\n",
    "    df['text'] = df['text'].progress_apply(data_cleaning)\n",
    "    df['tokenized text'] = df['text'].progress_apply(lambda document: word_tokenize(document.strip()))\n",
    "    df['clean_tokens'] = df['tokenized text'].progress_apply(lambda tokens: [token for token in tokens if token not in stop_words])\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Applying Lemmatization\n",
    "    df['lemmatized text'] = df['clean_tokens'].progress_apply(lambda tokens: ' '.join([lemmatizer.lemmatize(token, pos='v') for token in tokens]))\n",
    "\n",
    "# Creating new features\n",
    "    df['no_of_charcters'] = df['lemmatized text'].progress_apply(len)\n",
    "    df['no_of_words'] = df['lemmatized text'].progress_apply(lambda document: word_tokenize(document)).apply(len)\n",
    "    return df['lemmatized text']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = joblib.load('../models/logistic_regression_model.pkl')\n",
    "nb = joblib.load('../models/naive_bayes_model.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "text='Hi !! How are you ? Are you sad?'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 1003.18it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 495.31it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 1001.27it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 999.36it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 997.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    hi sad\n",
      "Name: lemmatized text, dtype: object\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(data_prep_txt(text))\n",
    "print(lr.predict(data_prep_txt(text)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>not good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>feeling bad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>how are you</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I am sick</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>bad day</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          text\n",
       "0     not good\n",
       "1  feeling bad\n",
       "2  how are you\n",
       "3    I am sick\n",
       "4      bad day"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('../Data/test_file.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:00<00:00, 5084.00it/s]\n",
      "100%|██████████| 5/5 [00:00<00:00, 2488.02it/s]\n",
      "100%|██████████| 5/5 [00:00<?, ?it/s]\n",
      "100%|██████████| 5/5 [00:00<00:00, 5136.30it/s]\n",
      "100%|██████████| 5/5 [00:00<00:00, 5085.24it/s]\n",
      "100%|██████████| 5/5 [00:00<00:00, 2495.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Positive' 'Negative' 'Positive' 'Negative' 'Negative']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(lr.predict(data_prep_series(df)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
